<html> <head> <style> @import url('https://fonts.googleapis.com/css2?family=DM+Mono:ital,wght@0,300;0,400;0,500;1,300;1,400;1,500&display=swap'); </style> <link rel="stylesheet" href="path/to/font-awesome/css/all.min.css"> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script> window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] } }; </script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="stylesheet" href="..\static\css\paper.css"> </head> <div class="toc"> </div> <article><h2 id=Econometrics>Econometrics</h2><p>There are 4 assumptions on the sample or data:</p><p>1. ** Random Sampling **: The sample is a random sample from the population. </p><p>2. ** Independence of Observations **: The values of one sample observation are not affected by the values of other observations.</p><p>3. ** Large Sample **: The sample is large enough to apply the Central Limit Theorem. This ensures normality if the residuals.</p><p>4. ** No Outliers **: The sample does not contain any outliers.</p><p>There are 3 assumptions on model specification:</p><p>1. ** Linear in Parameters **: The model is linear in the parameters.</p><p>2. ** No Multicollinearity **: The independent variables are not linearly dependent on each other.</p><p>3. ** No Endogeneity **: The independent variables are not correlated with the error term. This is also known as zero conditional mean of errors assumption.</p><p>There are 2 assumptions on the error term:</p><p>1. ** Constant Variance **: The variance of the error term is constant across all levels of the independent variables.</p><p>2. ** No Autocorrelation **: The error terms are not correlated with each other.</p><p>3. </p><h3 id=LinearityinParameters>Linearity in Parameters</h3><h4 id=DiagnosisofNon-Linearity>Diagnosis of Non-Linearity</h4><h5 id=1.ComponentplusResidualPlots>1. Component plus Residual Plots</h5><p>The component plus residual plot is a scatter plot of the residuals against the fitted values. If relationship between the residuals and the fitted values is linear, the plot will show no pattern. If the plot shows a pattern, it indicates </p><p>non-linearity in the model.</p><h4 id=PotentialSolutions>Potential Solutions</h4><h5 id=1.PolynomialRegression>1. Polynomial Regression</h5><p>Polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x. </p><h5 id=2.TransformationofVariables>2. Transformation of Variables</h5><p>Transforming the dependent variable or the independent variable can help in linearizing the relationship between the dependent and independent variables.</p><h3 id=Homoskedasticity>Homoskedasticity</h3><p>The observations are homoskedastic if the variance of the residuals are constant across all levels of the independent variables. </p><p>$$</p><p>\text{Var}(\epsilon) = \sigma^2 I</p><p>$$</p><p>In presence of heteroscedasticity, the variance-covariance matrix of the errors becomes:</p><p>$$</p><p>\text{Var}(\epsilon) = \sigma^2 \Sigma</p><p>$$</p><p>Where \(\Sigma\) is a diagonal matrix with non-constant elements.</p><h3 id=Standarderrorsofthecoefficients>Standard errors of the coefficients</h3><p>The standard error of the coefficients (σ(β)) can be calculated using the formula:</p><p>$$</p><p>\text{Var}(\hat{\beta}) = \sigma^2 (X'X)^{-1} </p><p>$$</p><p>In presence of heteroscedasticity, the standard errors of the coefficients become:</p><p>$$</p><p>\text{Var}(\hat{\beta}) = \sigma^2 (X' \Sigma X)^{-1}</p><p>$$</p><h5 id=DiagnosisofHeteroskedasticity>Diagnosis of Heteroskedasticity</h5><p>Write the errors in the regression model as a function of the independent variables and their products. Then, test the joint hypothesis that the coefficients of the squared residuals are jointly equal to zero. This is known as White's test for heteroscedasticity.</p><p>$$ \epsilon_i^2 = \gamma_0 + \gamma_1X_{1i} + \gamma_2X_{2i} + \ldots + \gamma_kX_{ki} + u_i $$</p><p>Here, \(Y_i\) is the dependent variable, \(X_{1i}, X_{2i}, \ldots, X_{ki}\) are the independent variables, \(\epsilon_i\) is the original residual, \(\epsilon_i^2\) is the squared residual, and \(u_i\) is the error term for the squared residuals.</p><p>The test statistic can be calculated as:</p><p>$$ LM = nR^2 $$</p><p>where \(n\) is the sample size and \(R^2\) is the \(R^2\) from the extended regression.</p><p>You can then compare the test statistic to the critical values from the chi-squared distribution to determine whether to reject the null hypothesis of homoscedasticity.</p><p>If the test suggests the presence of heteroscedasticity, you may need to consider transformations of variables or use robust standard errors to address the issue in your regression analysis. Keep in mind that other diagnostic tests and visual inspection of residual plots should also be considered in conjunction with White's test for a more comprehensive assessment.</p><h5 id=PotentialSolutions>Potential Solutions</h5><p>Variance stabilizing transformation: A transformation of the outcome $Y$ used to correct non-constant variance is called a “variance stabilizing transformation.”used to correct non-constant variance is called a “variance stabilizing transformation.” Again, common transformations are the natural logarithm, square root, inverse, and Box-Cox.</p><h3 id=NoMulticollinearity>No Multicollinearity</h3><p>The matrix of the independent variables should have full rank. This means that the independent variables should not be linearly dependent on each other. The presence of multicollinearity can cause large standard errors and unreliable p-values. This can make it difficult to determine which predictors are important.</p><p> </p><h5 id=DiagnosisofMulticollinearity>Diagnosis of Multicollinearity</h5><p>1. **Correlation Matrix**: A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. The value is in the range of -1 to 1. If two variables have high correlation, it means that they have a linear relationship.</p><p>2. **Variance Inflation Factor (VIF)**: The variance inflation factor (VIF) quantifies the severity of multicollinearity in an ordinary least squares regression analysis. It provides an index that measures how much the variance (the square of the estimate’s standard deviation) of an estimated regression coefficient is increased because of collinearity. A VIF value greater than 10 is a sign of multicollinearity.</p><h5 id=PotentialSolutions>Potential Solutions</h5><p>1. **Remove some of the highly correlated independent variables**: If you have two or more independent variables that are highly correlated, you can remove one of the variables. If you have a response variable that is highly correlated with one of the independent variables, you can remove the response variable.</p><p>2. **Combine the correlated independent variables**: If you have two or more independent variables that are highly correlated, you can combine them into a single variable. For example, if you have two variables that measure the same thing, you can average them.</p><p>3. **Use principal components**: Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables. The PCs are calculated using the eigenvectors of the correlation matrix of the original variables. The PCs are linear combinations of the original variables.  The eigenvectors associated with the eigenvalues represent the factor loadings in factor analysis. </p><h3 id=BiasinOLS>Bias in OLS</h3><p>Suppose that a correctly specified regression models is</p><p>$$\text{Wage} = \text{education}_{i} \ \beta_{1}+ \text{age}_{i} \ \beta_{2}+ u_i$$</p><p>The model specification implies that the wage of the individual is dependent on education level as well as on age. Let \(\text{education}\) be denoted by \(X_1\), \(\text{age}\) be denoted by \(X_2\) and \(\text{Wage}\) be denoted by \(Y\) , for the sake of simplicity of notation.Suppose we exclude the age of the individual from our model specification and only regress wage on the education. The sample regression function for the above specification is:</p><p>$$ Y = X_{1} \ \hat{\beta_{1}} + \epsilon $$</p><p>Out estimator for \(\beta_1\) is:</p><p>$$\hat{\beta_{1}}=\left(X_{1}^{\mathrm{T}} X_{1}\right)^{-1} X_{1}^{\mathrm{T}} Y$$</p><p>From the true specification we have \(Y = X_{1} \ \hat{\beta_{1}} + X_{2} \ \hat{\beta_{2}} \)</p><p>Therefore, </p><p>$$</p><p>\begin{aligned} </p><p>\hat{\beta_{1}} &= \left(X_{1}^{\mathrm{T}} X_{1}\right)^{-1} X_{1}^{\mathrm{T}} \ \dot \ \left(X_{1} \ \hat{\beta_{1}} + X_{2} \ \hat{\beta_{2}} + \epsilon \right) \\</p><p>&= \beta_{1}\ + \ \left(X_{1}^{\mathrm{T}} X_{1}\right)^{-1} X_{1}^{\mathrm{T}} X_{2} \  \beta_{2}\ + \ \left(X_{1}^{\mathrm{T}} X_{1}\right)^{-1} X_{1}^{\mathrm{T}} \ \epsilon</p><p>\end{aligned}</p><p>$$</p><p>Take Expectations,</p><p>$$</p><p>\begin{aligned} </p><p>\mathbf{E} \left[\hat{\beta_{1}}\right] = \beta_1 + \underbrace{\mathbf{E} \left[\left(X_{1}^{\mathrm{T}} X_{1}\right)^{-1} X_{1}^{\mathrm{T}} X_{2}\right] {\beta_{2}}}_\text{Bias}   </p><p>\end{aligned}</p><p> $$</p><p>Note that, \(\mathbf{E}[\epsilon]=0\)</p><p>This gives us:</p><p>$$E\left[b_{1} \mid X\right]=\beta_{1}+\rho_{1.2} \beta_{2} \quad \text { where } \quad \rho_{1.2}=\left(X_{1}^{\mathrm{T}} X_{1}\right)^{-1} X_{1}^{\mathrm{T}} X_{2}$$</p><h2 id=MachineLearning>Machine Learning</h2><h3 id=Bias-VarianceTradeoff>Bias-Variance Tradeoff</h3><p>The bias-variance tradeoff stands as a key challenge in supervised machine learning. A too-simple model might underfit that is it would not capture the signal and a too-complicated model might overfit that is it would make noisy predictions. In statistics, Bias is the difference between the expected value of an estimator (a statistical quantity calculated from the data) and the true underlying parameter being estimated.  </p><h4 id=Bias>Bias</h4><p>Mathematically, if $\hat{\theta}$ is an estimator for a parameter $\theta$, the bias ($\text{Bias}(\hat{\theta})$) is defined as</p><p>$$ \text{Bias}(\hat{\theta}) = \mathbf{E}[\hat{\theta}] - \theta $$</p><p>where $\mathbf{E}[\hat{\theta}]$ is the expected value (average value) of the estimator across all possible samples, and $\theta$ is the true parameter value.</p><h4 id=Variance>Variance</h4><p>Variance is the second moment about the mean. </p><p>$$ \sigma^2 = \mathbf{E}[(X - \mu)^2] $$</p><p>This formula represents the expected value of the squared differences between each observation \(X\) and the mean \(\mu\). The expectation is taken over all possible values of \(X\) according to the probability distribution of \(X\).</p><p>If \(X\) is a discrete random variable with probability mass function \(P(X = x_i)\) for each value \(x_i\), the formula becomes:</p><p>$$ \sigma^2 = \sum_{i} (x_i - \mu)^2 \cdot P(X = x_i) $$</p><p>If \(X\) is a continuous random variable with probability density function \(f(x)\), the formula becomes:</p><p>$$ \sigma^2 = \int_{-\infty}^{\infty} (x - \mu)^2 \cdot f(x) \,dx $$</p><p>In summary, the variance quantifies the dispersion or spread of the random variable's values around its mean.</p><h2 id=TimeSeriesAnalysis>Time Series Analysis</h2><h3 id=Stationarity>Stationarity</h3><p>A time series \( \{X_t\} \) is weakly stationary if it satisfies the following conditions for all time points \( t \), \( s \), and all time intervals \( h \):</p><p>1. **Constant Mean:**</p><p>   $$ \text{E}(X_t) = \mu $$</p><p>   The mean of the series is constant over time.</p><p>2. **Constant Variance:**</p><p>   $$ \text{Var}(X_t) = \sigma^2 $$</p><p>   The variance of the series is constant over time.</p><p>3. **Autocovariance is Time-Invariant:**</p><p>   $$ \text{Cov}(X_t, X_{t+h}) = \text{Cov}(X_{t+s}, X_{t+s+h}) $$</p><p>   The covariance between observations at different time points is only a function of the time lag \( h \), not the specific time point \( t \).</p><h3 id=ADFTest>ADF Test</h3><p>The Augmented Dickey-Fuller (ADF) test is a statistical test used to assess the presence of a unit root in a time series dataset. The presence of a unit root suggests that the time series is non-stationary, meaning that its statistical properties, such as mean and variance, are not constant over time. The ADF test is commonly used in econometrics and time series analysis. The null hypothesis of the ADF test is that a unit root is present in the time series, indicating non-stationarity. The alternative hypothesis is that the time series is stationary after differencing.</p><p>The statistical definition of the ADF test can be expressed as follows:</p><p>Consider the time series \( \{X_t\} \). The null hypothesis (\(H_0\)) and alternative hypothesis (\(H_1\)) of the ADF test are defined as:</p><p>$$ H_0: \text{The time series has a unit root (non-stationary)} $$</p><p>$$ H_1: \text{The time series is stationary after differencing} $$</p><p>The ADF test is often specified in the form of a regression model:</p><p>$$ \Delta X_t = \alpha + \beta t + \gamma X_{t-1} + \delta_1 \Delta X_{t-1} + \ldots + \delta_{p-1} \Delta X_{t-p+1} + \epsilon_t $$</p><p>where:</p><ul>\( \Delta X_t \) is the differenced series.</ul><ul>\( t \) is a time trend.</ul><ul>\( X_{t-1} \) is the lagged value of the original series.</ul><ul>\( \Delta X_{t-1}, \Delta X_{t-2}, \ldots, \Delta X_{t-p+1} \) are lagged differences.</ul><ul>\( \epsilon_t \) is the error term.</ul><p>The test statistic from this regression is compared to critical values from statistical tables to determine whether to reject the null hypothesis of a unit root. If the test statistic is more negative than the critical values, the null hypothesis is rejected, suggesting stationarity.</p><p># Assuming your time series data is stored in a variable 'data'</p><p># Extracting and printing the test statistic and p-value</p><code>import statsmodels.api as sm
result = sm.tsa.adfuller(data)
test_statistic, p_value, _, _, _, _ = result
print(f'Test Statistic: {test_statistic}')
print(f'P-value: {p_value}')
</code><h3 id=ConditionalHeteroskedasticity>Conditional Heteroskedasticity</h3><p>A collection of random variables is heteroskedastic if there are certain groups, or subsets, of variables within the larger set that have a different variance from the remaining variables. In finance, an increase in variance maybe correlated to a further increase in variance. For instance, on a day that equities markets undergo a substantial drop, automated risk management sell orders in long only portfolios get triggered, which lead to further fall in the price of equities within these portfolios, leading to significant downward volatility. These "sell-off" periods, as well as many other forms of volatility, lead to heteroskedasticity that is serially correlated and hence conditional on periods of increased variance. Thus we say that such series are conditional heteroskedastic.</p><h2 id=IFRS9>IFRS9</h2><p> ### What is Foundation IRB?</p><p>Banks own estimation of 1-year PD and regulators prescribed LGD and EAD. If EL > Provision then excess is reduced from capital. If EL < Provision then excess is added to capital</p><p> ### What is Advanced IRB Approach?</p><p>Advance IRB Approach: Banks own estimation of PD, LGD and EAD</p><p>Then total required capital is calculated as a fixed percentage of the estimated RWA.</p><p> ### What is ICAAP?</p><p>To inform the board of directors of the ongoing assessment of the bank’s risk and how the bank intends to mitigate those risks. Also, to assess the current and future capital requirements.</p><p> ### What is Tier 1 Capital?</p><p>It is also called core capital.</p><ul>It consists of bank’s equity capital and disclosed reserves.</ul><ul>Minimum tier 1 capital is 6% of RWAs. It is Equals shareholder’s equity + retained earnings.</ul><ul>It has two components: Common Equity Tier 1 (CET1) and Additional Tier 1 (AT1) capital.</ul><ul>Under the new guidelines, the minimum CET1 capital ratio was set at 4.5%, and the minimum Tier 1 capital ratio (CET1 + AT1) was set at 6%. The total amount of reserve capital (Tier 1 and Tier 2) must be over 8%.</ul><p> ### What is Tier 2 Capital?</p><ul>Minimum tier 1 + tier 2 capital is 8% of RWAs.</ul><ul>Minimum capital adequacy ratio (including the capital conservation buffer) is 10.5% of RWAs.</ul><ul>It consists of Revaluation reserves + hybrid capital instruments + subordinate term debt + general loan loss reserves + undisclosed reserves, etc</ul><p> ### What is Tier 3 Capital?</p><p>Tier 3 capital debt used to include a greater number of subordinated issues when compared with tier 2 capital. Subordinated debt falls under other debt in payout priority if the borrower defaults. Subordinated debt is generally unsecured, meaning there is no collateral for the debt, so the issuer is left to trust that the borrower will pay them back.</p><p> ### What is Additional Tier 1 Capital?</p><p>Additional Tier 1 capital is defined as instruments that are not common equity but are eligible for inclusion in this tier. An example of AT1 capital is a contingent convertible or hybrid security, which has a perpetual term and can be converted into equity when a trigger event occurs.</p><p> ### What is Common Equity Tier 1 (CET1) Capital?</p><p>Common Equity Tier 1 is "the highest quality of regulatory capital, as it absorbs losses immediately when they occur," according to the Bank of International Settlements. A bank's Tier 1 capital must include a minimum ratio of 4.5% of CET1 to its RWAs. Bank for International Settlements. "Definition of capital in Basel III – Executive Summary," </p><p> ### What is CET 1 Ratio?</p><p>The CET1 ratio is a key measure of a bank's financial strength that is defined as the ratio of a bank's core equity capital to its total risk-weighted assets (RWA). It is a key metric of a bank's financial strength that has been adopted by the Basel Committee for Banking Supervision as a minimum standard.</p><p> ### What is RWA?</p><p>Banks calculate risk-weighted assets by multiplying the exposure amount by the relevant risk weight for the type of loan or asset. </p><ul>There are two components: Risk Weights and Expoosure</ul></article> <script>let tableOfContents = [{"heading":"Econometrics","sub_sections":["Linearity in Parameters","Homoskedasticity","Standard errors of the coefficients","No Multicollinearity","Bias in OLS"]},{"heading":"Machine Learning","sub_sections":["Bias-Variance Tradeoff"]},{"heading":"Time Series Analysis","sub_sections":["Stationarity","ADF Test","Conditional Heteroskedasticity"]},{"heading":"IFRS9","sub_sections":[]}]</script> <script language="JavaScript" type="text/javascript " src="..\static\js\toc.js"></script> </html>